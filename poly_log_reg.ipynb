{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача: реализовать пакетный градиентный спуск с ранним прекращением для полиномиальной логистической регрессии (не используя Scikit-Learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для начала разберемся с терминами  \n",
    "\n",
    "*Полиномиальная логистическая регрессия*  \n",
    "\n",
    "Для начала, разберемся, что такое логистическая регрессия  \n",
    "\n",
    "Логистическая регрессия или логит-регрессия - алгоритм регрессии, который может применяться для задач классификации. Он используется для оценки вероятности принадлежности экземпляра к определенному классу. Логистическая регрессионная модель подсчитывает взвешенные суммы входных признаков и выдает логистику результата.\n",
    "\n",
    "Векторизированная форма для оценки вероятности принадлежности объекта к данному классу\n",
    "\\begin{align}\n",
    "\\hat{p}=h_\\theta(x)=\\sigma(\\theta^T\\cdotp x)\n",
    "\\end{align}\n",
    "\n",
    "Это сигмоидальная (S-образная) функция, которая выдает значения на интервале от 0 до 1 и определяется выражением.\n",
    "\\begin{align}\n",
    "\\sigma(t)=\\frac{1}{1+exp(-t)}\n",
    "\\end{align}\n",
    "\n",
    "прогноз логистической регрессионной модели:\n",
    "\\begin{align}\n",
    "\\hat{y}=\\begin{cases} 0,\\; if\\; \\hat{p} < 0.5,\\\\\n",
    "1,\\; if\\; \\hat{p} \\geq 0.5. \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Функция издержек одиночного обучающего образца\n",
    "\\begin{align}\n",
    "c(\\theta)=\\begin{cases} -log(\\hat{p}),\\; if\\;y=1,\\\\\n",
    "-log(1-\\hat{p}),\\; if\\; y=0. \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "А когда логит-регрессия становится полиномиальной?  \n",
    "\n",
    "Логистическая регрессионная модель может быть обобщена для поддержки множества классов напрямую, без необходимости в обучении и комбинировании многочисленных двоичных классификаторов. В результате получается полиномиальная или многопеременная логистическая регрессия.\n",
    "Имея образец $x$, полиномиальная логистическая регрессионная модель сначала вычисляет сумму очков $s_k(x)$ для каждого класса $k$, а затем оценивает вероятность каждого класса, применяя к суммам очков полиномиальную логистическую функцию (softmax), также называемую нормализованной экспоненциальной (normalized exponential).  \n",
    "Полиномиальная логистическая сумма очков для класса $k$:\n",
    "\\begin{align}\n",
    "s_k(x)=(\\theta^{(k)})^T\\cdotp x\n",
    "\\end{align}\n",
    "\n",
    "После получения вектора для каждого класса образца $x$ оценивается вироятность принадлежности к классу $k$ - она вычиляет экспоненту каждой суммы очков и затем нормализует их:\n",
    "\\begin{align}\n",
    "\\hat{p}_k=\\sigma(s(x))_k=\\frac{exp(s_k(x))}{\\sum_{j=1}^kexp(s_j(x))}\n",
    "\\end{align}\n",
    "где:  \n",
    "$K$ - количество классов;  \n",
    "$s(x)$ - вектор, содержащий суммы очков каждого класса для образца x;\n",
    "$\\sigma(s(x))_k$ - оценочная вероятность того, что образец $x$ принадлежит классу $k$ при заданных суммах очков каждого класса этог образца.\n",
    "\n",
    "Подобно классификатору, основанному на логистической регрессии, классификатор на базе многопеременной логистической регрессии прогнозирует класс с наивысшей оценочной вероятностью (который является просто классом с самой высокой суммой очков:\n",
    "\\begin{align}\n",
    "\\hat{y} = \\arg\\max_k\\sigma(s(x))_k=\\arg\\max_ks_k(x)=\\arg\\max_k((\\theta^{(k)})^T\\cdotp x)\n",
    "\\end{align}\n",
    "\n",
    "Операция argmax возвращает значение переменной, которая обращает функцию в максимум.  \n",
    "Классификатор на полиномальной логистической регрессии прогнозирует только один класс за раз, поэтому должен использоваться только с взаимоисключающими классами.  \n",
    "\n",
    "\n",
    "В качестве функции издержек используется функция перекрестной энтропии (кросс-энотропии), которая страфует модель, когда та дает оценку в виде низкой вероятности для целевого класса:\n",
    "\\begin{align}\n",
    "J(\\Theta) = -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^Ky_k^{(i)}log(\\hat{p}_k^{(i)})\n",
    "\\end{align}\n",
    "где:\n",
    "$\\Theta$ - матрица параметров.  \n",
    "\n",
    "Значение $y_k^{(i)}$ равно 1, если целевым классом для i-того образца явлется k, иначе оно равно 0.\n",
    "\n",
    "Вектор-градиент перекрестной энтропии для класса k\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta^{(k)}}J(\\Theta)=\\frac{1}{m}\\sum_{i=1}^m(\\hat{p}_k^{(i)}-y_k^{(i)})x^{(i)}\n",
    "\\end{align}\n",
    "\n",
    "*Пакетный градиентный спуск*  \n",
    "Градиентный спуск - общий алгоритм оптимизации, способный находить оптимальные решения широкого диапазона задач. Основная идея градиентного спуска - итеративно подстраивать параметры для сведения к минимуму функции издержек.\n",
    "Пакетный градиентный спуск вычисляет на каждом шаге обучения градиент для всей выборке обучающих данных.\n",
    "Градиентный спуск измеяет локальный градиент функции ошибки применительно к вектору параметров $\\theta$ и двигается в направлении убывающего градиента. Как только градиент становится нулевым - значит достигнут минимум.  \n",
    "Реализация выглядит следующим образом: инициализируется случайный вектор $\\theta$. Затем итеративным образом этот вектор обновляется для снижения функции издержек, до тех пор, пока алгоритм не сойдется в минимуме.  \n",
    "Размер \"шагов\" определяется гиперпараметром - скоростью обучения.\n",
    "\n",
    "Значение параметра:\n",
    "- слишком маленькое значение делает алгоритм медленным и вычислительно дорогим;\n",
    "- слишком большое значение может привести к тому, что градиентный спуск проскочи минимум или даже будет расходиться.\n",
    "\n",
    "Главная проблема в том, что мы не знаем топологию простанства функции ошибок, так что при неудачной инициализации и неправильном шаге обучения, функция может не найти глобальный минимум и застрять на плато или свалиться в локальный минимум.\n",
    "\n",
    "Хорошо, если функци выпуклая и непрерывная.\n",
    "\n",
    "Частная функция для производной функции издержек:\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta_j}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^m(\\theta^T\\cdotp x^{(i)}-y^{(i)})x_j^{(i)}\n",
    "\\end{align}\n",
    "\n",
    "Вектор-градиент функции издержек:\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta}MSE(\\theta) = \\begin{pmatrix}\n",
    "\\frac{\\partial}{\\partial \\theta_0}MSE(\\theta) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_1}MSE(\\theta) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_2}MSE(\\theta) \\\\\n",
    "... \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_n}MSE(\\theta) \n",
    "\\end{pmatrix} = \\frac{2}{m}X^T\\cdotp (X\\cdotp \\theta - y)\n",
    "\\end{align}\n",
    "С вектором-градиентом функции издержек работает алгоритм, который называется пакетным градиентным спуском. Он на каждом шаге потребляет всю выборку обучающих данных.  \n",
    "\n",
    "Шаг градиентного спуска:\n",
    "\\begin{align}\n",
    "\\theta^{t+1} = \\theta^t - \\eta\\nabla_\\theta MSE(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "*Раннее прекращение*  \n",
    "\n",
    "Раннее прекращение - обучение заканчивается, как только ошибка проверки достигает какого-то заранее опредлеенного минимума. В случае использование стохастичиского или мини-пакетного градиентного спуска хорошим решением может быть прекращение обучения после достижения определенного уровня ошибки и сохранения ее на протяжении определенного числа итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Что нужно сделать, чтобы реализовать алгоритм:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создать случайную выборку\n",
    "import numpy as np\n",
    "X = np.random.normal(0, 20, (500, 4))\n",
    "y = np.random.randint(0, 3, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.98360771  0.10249044  0.79090348]\n",
      " [-0.48348054  1.32470913  1.33417594]\n",
      " [ 0.17749465  0.92874324  1.01648191]\n",
      " [ 0.05771686 -0.30772955 -2.26498791]]\n"
     ]
    }
   ],
   "source": [
    "# инициализируем начальные веса\n",
    "theta = np.random.randn(4,3)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# логистическая регрессия\n",
    "def logit(x:float) -> float:\n",
    "    return(1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_probability(theta:np.ndarray, x:np.ndarray) -> np.ndarray:\n",
    "    s = np.dot(theta.T, x)\n",
    "    p = logit(s)\n",
    "    return(argmax(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(p:np.ndarray, y_real:np.ndarray) -> float:\n",
    "    cost = (-1/500) *((y_real * np.log(p)) +((1 - y_real) * np.log(1-p)))\n",
    "    total = cost.sum(axis = 0)\n",
    "    cost_tot = np.concatenate((cost_tot, tot), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задать гиперпараметры\n",
    "n_epochs = 1000\n",
    "eps = 10e-8\n",
    "stop_iter_epoch = 20\n",
    "eta = 0.1\n",
    "cost_tot = np.empty((0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-18.33656825   3.70015593   7.36283055   0.53265935]\n",
      " [  0.69639636  -3.75033067  32.21554467  15.93393174]\n",
      " [  7.49030233  16.59109903  20.84258762 -15.66441528]\n",
      " ...\n",
      " [-17.06393463  24.3622098   12.13766851 -25.09099653]\n",
      " [-11.04252674 -33.19810189  11.0844995  -22.07610823]\n",
      " [ -6.70149948  37.66717478 -24.99432476  15.70556806]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
